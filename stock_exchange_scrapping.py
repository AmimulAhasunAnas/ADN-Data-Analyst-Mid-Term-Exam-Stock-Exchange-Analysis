# -*- coding: utf-8 -*-
"""Stock Exchange Scrapping.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10BUqrOv53leGQ6h5e7rapt5gGRuQfcie
"""

import re
import requests
from bs4 import BeautifulSoup
import csv
import random
from itertools import count
import itertools
import pandas as pd

page = requests.get("https://www.dsebd.org/company_listing.php")
soup = BeautifulSoup(page.content, 'html.parser')

# Create company_names as an empty list

codes1=[]
# Find all the company names
trading = soup.find_all("div", {"class": "BodyContent"})
codes=soup.find_all("a", {"class": "ab1"})
for i in trading:
  code=i.find_all("a")
  for only_code in code:
    c=only_code.text.strip()
    codes1.append(c)

element_to_remove = 'More...'

trading_codes = [x for x in codes1 if x != element_to_remove]

print(len(trading_codes))

start_index = 381
end_index = 622

trading_codes = [x for i, x in enumerate(trading_codes) if i < start_index or i > end_index]

print(len(trading_codes))

from types import NoneType
scrip_codes = []
for i in range(0, len(trading_codes)):
    url = f'https://www.dsebd.org/displayCompany.php?name={trading_codes[i]}'
    response = requests.get(url)
    html_content = response.content
    soup = BeautifulSoup(html_content, "html.parser")
    tag_value = soup.find("tr", {"class": "alt"})
    try:
        if tag_value is not None:
            scrip_code = tag_value.find_all('th')
            scrip = scrip_code[1].text
            sl = slice(11, 17)
            scrip_codes.append(scrip[sl])
        else:
            print("Tag value not found for trading code:", trading_codes[i])
    except TypeError:
        print()

print(len(scrip_codes))

urls=[]
for i in range(0,len(trading_codes)):
    url=f'https://www.dsebd.org/displayCompany.php?name={trading_codes[i]}'
    urls.append(url)

print(urls)

page = requests.get("https://www.dsebd.org/company_listing.php")
soup = BeautifulSoup(page.content, 'html.parser')

# Create company_names as an empty list

company_name=[]
# Find all the company names
trading = soup.find_all("div", {"class": "BodyContent"})
comp=soup.find_all("span")
for i in trading:
  comp=i.find_all("span")
  for only_comp in comp:
    c=only_comp.text.strip()
    company_name.append(c)

print(company_name)

start_index = 380
end_index = 621

company_name = [x for i, x in enumerate(company_name) if i < start_index or i > end_index]

print(company_name)

company_names=[]
for i in company_name:
  compname= re.findall(r'\((.*?)\)', i)
  company_names.append(compname)

print(company_names)

company_names = list(itertools.chain(*company_names))
print(company_names)

sectors=[]
for i in range(0,len(trading_codes)):
    url=f'https://www.dsebd.org/displayCompany.php?name={trading_codes[i]}'
    response = requests.get(url)
    html_content = response.content
    soup = BeautifulSoup(html_content, "html.parser")
    tag_value = soup.find_all("div", {"class": "table-responsive"})
    tr=tag_value[2].find_all("tr", {"class": "alt"})
    td=tr[1].find_all("td")
    sectors.append(td[1].text)
print(sectors)

websites=[]
for i in range(0,len(trading_codes)):
    url=f'https://www.dsebd.org/displayCompany.php?name={trading_codes[i]}'
    response = requests.get(url)
    html_content = response.content
    soup = BeautifulSoup(html_content, "html.parser")
    tag_value = soup.find_all("div", {"class": "table-responsive"})
    tr=tag_value[11].find_all("tr")
    td=tr[5].find_all("td")
    a=td[1].find("a", {"class": "ab1"})
    websites.append(a.text)
print(websites)

counter = count(start=1)  # Start counting from 1
uid=[]
def generate_unique_id(text):
    serial_number = next(counter)
    random_number = random.randint(1000, 9999)
    unique_id = f"{text}-{serial_number}-{random_number}"
    return unique_id

# Example usage
text = "DSE"
for _ in range(len(trading_codes)):
    unique_id = generate_unique_id(text)
    uid.append(unique_id)
print(uid)

data = {'Company_ID': uid, 'Company_Name': company_names,'Sectors': sectors,'Trading_Codes':trading_codes,'Scrip_Codes':scrip_codes,'Websites':websites,'Urls':urls}

df = pd.DataFrame(data)

print(df)

df.to_csv('company_data.csv', index=False)

otherinfo=[]
for i in range(0,len(trading_codes)):
    url=f'https://www.dsebd.org/displayCompany.php?name={trading_codes[i]}'
    response = requests.get(url)
    html_content = response.content
    soup = BeautifulSoup(html_content, "html.parser")
    tag_value = soup.find_all("div", {"class": "table-responsive"})
    tr=tag_value[9].find_all("tr")
    sli=slice(54,66)
    try:
      if(tr[3] in tr):
        tr1=tr[3].find_all("td")
        date=tr1[0].text
        float_numbers = re.findall(r'\d+\.\d+',tr1[1].text)
        sponsor,govt,institute,foreign,public=float_numbers
        otherinfo.append([uid[i],date[sli],sponsor,govt,institute,foreign,public])
      if (tr[5] in tr):
        tr2=tr[5].find_all("td")
        date=tr2[0].text
        float_numbers = re.findall(r'\d+\.\d+',tr2[1].text)
        if len(float_numbers)==5:
          sponsor,govt,institute,foreign,public=float_numbers
          otherinfo.append([uid[i],date[sli],sponsor,govt,institute,foreign,public])
      if (tr[7] in tr):
        tr3=tr[7].find_all("td")
        date=tr3[0].text
        float_numbers = re.findall(r'\d+\.\d+',tr3[1].text)
        if len(float_numbers)==5:
          sponsor,govt,institute,foreign,public=float_numbers
          otherinfo.append([uid[i],date[sli],sponsor,govt,institute,foreign,public])
    except IndexError:
        print("")

for i in otherinfo:
  print(i)

column_labels = ['Company_ID','Date','Sponsor', 'Govt','Institute','Foreign','Public']
df2 = pd.DataFrame(otherinfo, columns=column_labels)

print(df2)

df2.to_csv('Holding_data.csv', index=False)