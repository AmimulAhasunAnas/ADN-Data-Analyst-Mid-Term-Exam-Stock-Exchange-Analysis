# -*- coding: utf-8 -*-
"""Stock Exchange with Schduler.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YofeFAlwYvjBdeOa9rxG6QO1qPq_IIY1
"""

!pip install schedule

import re
import requests
from bs4 import BeautifulSoup
import csv
import random
from itertools import count
import itertools
import pandas as pd
import schedule
import time
from datetime import datetime
import pytz

def datasets():
    page = requests.get("https://www.dsebd.org/company_listing.php")
    soup = BeautifulSoup(page.content, 'html.parser')

    codes1 = []
    trading = soup.find_all("div", {"class": "BodyContent"})
    codes = soup.find_all("a", {"class": "ab1"})
    for i in trading:
        code = i.find_all("a")
        for only_code in code:
            c = only_code.text.strip()
            codes1.append(c)

    element_to_remove = 'More...'
    trading_codes = [x for x in codes1 if x != element_to_remove]
    start_index = 380
    end_index = 621
    trading_codes = [x for i, x in enumerate(trading_codes) if i < start_index or i > end_index]

    uid = []
    counter = count(start=1)
    text = "DSE"
    for _ in range(len(trading_codes)):
        serial_number = next(counter)
        random_number = random.randint(1000, 9999)
        unique_id = f"{text}-{serial_number}-{random_number}"
        uid.append(unique_id)

    scrip_codes = []
    urls = []
    sectors = []
    websites = []
    oinfo = []
    for i in range(0, len(trading_codes)):
        url = f'https://www.dsebd.org/displayCompany.php?name={trading_codes[i]}'
        response = requests.get(url)
        html_content = response.content
        soup = BeautifulSoup(html_content, "html.parser")
        tag_value = soup.find("tr", {"class": "alt"})
        if tag_value is not None:
            scrip_code = tag_value.find_all('th')
            scrip = scrip_code[1].text
            sl = slice(11, 17)
            scrip_codes.append(scrip[sl])
            urls.append(url)
            tag_value = soup.find_all("div", {"class": "table-responsive"})
            if len(tag_value) >= 3:
                tr = tag_value[2].find_all("tr", {"class": "alt"})
                if len(tr) > 1:
                    td = tr[1].find_all("td")
                    sectors.append(td[1].text)
                    tag_value2 = soup.find_all("div", {"class": "table-responsive"})
                    if len(tag_value2) >= 12:
                        tr = tag_value2[11].find_all("tr")
                        if len(tr) > 5:
                            td = tr[5].find_all("td")
                            a = td[1].find("a", {"class": "ab1"})
                            websites.append(a.text)
                            tag_value3 = soup.find_all("div", {"class": "table-responsive"})
                            if len(tag_value3) >= 10:
                                tr = tag_value3[9].find_all("tr")
                                if len(tr) > 3 and len(tr) > 5 and len(tr) > 7:
                                    sli = slice(54, 66)
                                    tr1 = tr[3].find_all("td")
                                    date = tr1[0].text
                                    float_numbers = re.findall(r'\d+\.\d+', tr1[1].text)
                                    if len(float_numbers) == 5:
                                        sponsor, govt, institute, foreign, public = float_numbers
                                        oinfo.append(
                                            [uid[i], date[sli], sponsor, govt, institute, foreign, public])
                                    tr2 = tr[5].find_all("td")
                                    date = tr2[0].text
                                    float_numbers = re.findall(r'\d+\.\d+', tr2[1].text)
                                    if len(float_numbers) == 5:
                                        sponsor, govt, institute, foreign, public = float_numbers
                                        oinfo.append(
                                            [uid[i], date[sli], sponsor, govt, institute, foreign, public])
                                    tr3 = tr[7].find_all("td")
                                    date = tr3[0].text
                                    float_numbers = re.findall(r'\d+\.\d+', tr3[1].text)
                                    if len(float_numbers) == 5:
                                        sponsor, govt, institute, foreign, public = float_numbers
                                        oinfo.append(
                                            [uid[i], date[sli], sponsor, govt, institute, foreign, public])
        else:
            print("")

    company_name = []
    page1 = requests.get("https://www.dsebd.org/company_listing.php")
    soup1 = BeautifulSoup(page1.content, 'html.parser')
    tradingx = soup1.find_all("div", {"class": "BodyContent"})
    comp = soup1.find_all("span")
    for i in tradingx:
        comp = i.find_all("span")
        for only_comp in comp:
            c = only_comp.text.strip()
            company_name.append(c)
    start_index = 381
    end_index = 622
    company_name1 = [x for i, x in enumerate(company_name) if i < start_index or i > end_index]
    company_names = []
    for i in company_name1:
        compname = re.findall(r'\((.*?)\)', i)
        company_names.append(compname)
    company_names = list(itertools.chain(*company_names))

    data = {'Company_ID': uid, 'Company_Name': company_names, 'Sectors': sectors, 'Trading_Codes': trading_codes,
            'Scrip_Codes': scrip_codes, 'Websites': websites, 'Urls': urls}
    df = pd.DataFrame.from_dict(data, orient='index')
    df = df.transpose()
    df.to_csv('company_data.csv', index=False)

    column_labels = ['Company_ID', 'Date', 'Sponsor', 'Govt', 'Institute', 'Foreign', 'Public']
    df2 = pd.DataFrame(oinfo, columns=column_labels)
    df2.to_csv('Holding_data.csv', index=False)

datasets()

def schedule_task():
    # Set the timezone
    timezone = pytz.timezone('Asia/Dhaka')
    task_time = datetime.now(timezone).replace(hour=17, minute=0, second=0)
    schedule.every().day.at(task_time.strftime("17:00")).do(datasets)

    while True:
        schedule.run_pending()
        time.sleep(1)

schedule_task()